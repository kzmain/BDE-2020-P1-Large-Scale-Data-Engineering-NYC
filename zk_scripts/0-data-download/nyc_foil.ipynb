{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, TimestampType, DoubleType, ShortType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up modes and dirs\n",
    "overwrite  = False\n",
    "databricks = False\n",
    "if not databricks:\n",
    "    data_dir = '../data'\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "else:\n",
    "    data_dir = \"/dbfs/mnt/group01\"\n",
    "\n",
    "foil_dir = foil_dbfs = os.path.join(data_dir, \"foil\")\n",
    "down_dir = down_dbfs = os.path.join(foil_dir, \"down\")\n",
    "csvs_dir = csvs_dbfs = os.path.join(foil_dir, \"csv\")\n",
    "zips_dir = zips_dbfs = os.path.join(foil_dir, \"zip\")\n",
    "raws_dir = raws_dbfs = os.path.join(foil_dir, \"raw\")\n",
    "\n",
    "if databricks:\n",
    "    foil_dbfs = foil_dbfs.replace(\"/dbfs\", \"\")\n",
    "    down_dbfs = down_dbfs.replace(\"/dbfs\", \"\")\n",
    "    csvs_dbfs = csvs_dbfs.replace(\"/dbfs\", \"\")\n",
    "    zips_dbfs = zips_dbfs.replace(\"/dbfs\", \"\")\n",
    "    raws_dbfs = raws_dbfs.replace(\"/dbfs\", \"\")\n",
    "dirs = [data_dir, foil_dir, down_dir, csvs_dir, zips_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for d in dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "year_start = 2010\n",
    "year_end   = 2013\n",
    "moth_start = 1\n",
    "moth_end   = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_file_exist(_path):\n",
    "    if os.path.exists(_path) and not overwrite:\n",
    "            print(\"[SYSTEM]: File exists: {}\".format(_path))\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_foil_data():\n",
    "    info_title = lambda _file_name : print(\"____________________________FOIL_DOWNLOAD_{}____________________________\".format(_file_name))\n",
    "    info_start = lambda _file_name : print(\"[SYSTEM]: Start  {}\".format(_file_name))\n",
    "    info_end   = lambda _file_name : print(\"[SYSTEM]: Finish {}\".format(_file_name))\n",
    "\n",
    "    # Start to download the FOIL files\n",
    "    data_page = \"https://databank.illinois.edu/datasets/IDB-9610843\"\n",
    "    down_page = \"https://databank.illinois.edu/datafiles/{}/download\"\n",
    "\n",
    "    requ = requests.get(data_page)\n",
    "    resp = json.loads(requ.text)\n",
    "\n",
    "    for datafile in resp['datafiles']:\n",
    "        # databricks storage location\n",
    "        # remote download location\n",
    "        local_path = os.path.join(down_dir, datafile[\"binary_name\"])\n",
    "        remot_path = down_page.format(datafile[\"web_id\"])\n",
    "\n",
    "        info_title(datafile[\"binary_name\"])\n",
    "        if check_file_exist(local_path):\n",
    "            continue\n",
    "        # start to download the FOIL data\n",
    "        info_start(local_path)\n",
    "        urllib.request.urlretrieve(remot_path, local_path)\n",
    "        info_end(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________FOIL_DOWNLOAD_decompress.py____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/decompress.py\n",
      "____________________________FOIL_DOWNLOAD_FOIL2012.zip____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/FOIL2012.zip\n",
      "____________________________FOIL_DOWNLOAD_FOIL2011.zip____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/FOIL2011.zip\n",
      "____________________________FOIL_DOWNLOAD_New_York_City_Taxi_Data_2010-2013.pdf____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/New_York_City_Taxi_Data_2010-2013.pdf\n",
      "____________________________FOIL_DOWNLOAD_FOIL2010.zip____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/FOIL2010.zip\n",
      "____________________________FOIL_DOWNLOAD_FOIL2013.zip____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/down/FOIL2013.zip\n"
     ]
    }
   ],
   "source": [
    "download_foil_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_foil_down():\n",
    "    info_title = lambda _file_name : print(\"____________________________FOIL_EXTRACT_{}____________________________\".format(_file_name))\n",
    "    info_start = lambda _file_name : print(\"[SYSTEM]: Start  {}\".format(_file_name))\n",
    "    info_end   = lambda _file_name : print(\"[SYSTEM]: Finish {}\".format(_file_name))\n",
    "    zip_files = glob.glob(os.path.join(down_dir, \"*.zip\"))\n",
    "    # zip_files = glob.glob(os.path.join(down_dir, \"FOIL2011.zip\"))\n",
    "    for zip_file in zip_files:\n",
    "        target_folder = os.path.join(zips_dir, zip_file.replace(down_dir + \"/\", \"\").replace(\".zip\", \"\"))\n",
    "        if check_file_exist(target_folder):\n",
    "            continue\n",
    "        info_title(zip_file.replace(down_dir + \"/\", \"\"))\n",
    "        command = \"cd {} && jar -xvf {}\".format(zips_dir, zip_file)\n",
    "        info_start(zip_file.replace(down_dir + \"/\", \"\"))\n",
    "        os.system(command)\n",
    "        info_end(zip_file.replace(down_dir + \"/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________FOIL_EXTRACT_FOIL2012.zip____________________________\n",
      "[SYSTEM]: Start  FOIL2012.zip\n",
      "[SYSTEM]: Finish FOIL2012.zip\n",
      "____________________________FOIL_EXTRACT_FOIL2013.zip____________________________\n",
      "[SYSTEM]: Start  FOIL2013.zip\n",
      "[SYSTEM]: Finish FOIL2013.zip\n",
      "____________________________FOIL_EXTRACT_FOIL2011.zip____________________________\n",
      "[SYSTEM]: Start  FOIL2011.zip\n",
      "[SYSTEM]: Finish FOIL2011.zip\n",
      "____________________________FOIL_EXTRACT_FOIL2010.zip____________________________\n",
      "[SYSTEM]: Start  FOIL2010.zip\n",
      "[SYSTEM]: Finish FOIL2010.zip\n"
     ]
    }
   ],
   "source": [
    "extract_foil_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_foil_zip():\n",
    "    info_title = lambda _file_name : print(\"____________________________FOIL_ZIP_EXTRACT_{}____________________________\".format(_file_name))\n",
    "    info_start = lambda _file_name : print(\"[SYSTEM]: Start {}\".format(_file_name))\n",
    "    info_end   = lambda _file_name : print(\"[SYSTEM]: Finish {}\".format(_file_name))\n",
    "\n",
    "    for _year in range(year_start, year_end + 1):\n",
    "        zip_foil = os.path.join(zips_dir, \"FOIL{}\".format(_year))\n",
    "        zip_files = glob.glob(os.path.join(zip_foil, \"*.zip\"))\n",
    "        tgt_foil = os.path.join(csvs_dir, \"{}\".format(_year))\n",
    "\n",
    "        info_title(_year)\n",
    "        for zip_file in zip_files:\n",
    "            if not os.path.exists(tgt_foil):\n",
    "                os.makedirs(tgt_foil)\n",
    "\n",
    "            command = \"cd {} && jar -xvf {}\".format(tgt_foil, zip_file)\n",
    "            tar_file = zip_file\\\n",
    "                .replace(zips_dir + \"/\", \"\")\\\n",
    "                .replace(\"FOIL{}/\".format(_year), \"\")\n",
    "            if check_file_exist(os.path.join(tgt_foil, tar_file).replace(\"zip\", \"csv\")):\n",
    "                continue\n",
    "            info_start(tar_file)\n",
    "            os.system(command)\n",
    "            info_end(tar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________FOIL_ZIP_EXTRACT_2010____________________________\n",
      "____________________________FOIL_ZIP_EXTRACT_2011____________________________\n",
      "____________________________FOIL_ZIP_EXTRACT_2012____________________________\n",
      "____________________________FOIL_ZIP_EXTRACT_2013____________________________\n"
     ]
    }
   ],
   "source": [
    "extract_foil_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_trip(_in_df):\n",
    "    return _in_df\\\n",
    "        .withColumn(\"medallion\", col(\"medallion\").cast(IntegerType()))\\\n",
    "        .withColumn(\"hack_license\", col(\" hack_license\").cast(IntegerType()))\\\n",
    "        .drop(\" hack_license\")\\\n",
    "        .withColumn(\"pickup_datetime\", col(\" pickup_datetime\").cast(TimestampType()))\\\n",
    "        .drop(\" pickup_datetime\")\\\n",
    "        .withColumn(\"dropoff_latitude\", col(\" dropoff_latitude\").cast(DoubleType()))\\\n",
    "        .drop(\" dropoff_latitude\")\\\n",
    "        .withColumn(\"dropoff_longitude\", col(\" dropoff_longitude\").cast(DoubleType()))\\\n",
    "        .drop(\" dropoff_longitude\")\\\n",
    "        .withColumn(\"pickup_latitude\", col(\" pickup_latitude\").cast(DoubleType()))\\\n",
    "        .drop(\" pickup_latitude\")\\\n",
    "        .withColumn(\"pickup_longitude\", col(\" pickup_longitude\").cast(DoubleType()))\\\n",
    "        .drop(\" pickup_longitude\")\\\n",
    "        .withColumn(\"trip_distance\", col(\" trip_distance\").cast(DoubleType()))\\\n",
    "        .drop(\" trip_distance\")\\\n",
    "        .withColumn(\"trip_time_in_secs\", col(\" trip_time_in_secs\").cast(IntegerType()))\\\n",
    "        .drop(\" trip_time_in_secs\")\\\n",
    "        .withColumn(\"dropoff_datetime\", col(\" dropoff_datetime\").cast(TimestampType()))\\\n",
    "        .drop(\" dropoff_datetime\")\\\n",
    "        .withColumn(\"rate_code\", col(\" rate_code\").cast(ShortType()))\\\n",
    "        .drop(\" rate_code\")\\\n",
    "        .drop(\" passenger_count\")\\\n",
    "        .drop(\" vendor_id\")\\\n",
    "        .drop(\" store_and_fwd_flag\")\n",
    "\n",
    "def process_fare(_in_df):\n",
    "    return _in_df.withColumn(\"medallion\", col(\"medallion\").cast(IntegerType()))\\\n",
    "    .withColumn(\"hack_license\", col(\" hack_license\").cast(IntegerType()))\\\n",
    "    .drop(\" hack_license\")\\\n",
    "    .withColumn(\"pickup_datetime\", col(\" pickup_datetime\").cast(TimestampType()))\\\n",
    "    .drop(\" pickup_datetime\")\\\n",
    "    .withColumn(\"tip_amount\", col(\" tip_amount\").cast(DoubleType()))\\\n",
    "    .drop(\" tip_amount\")\\\n",
    "    .withColumn(\"total_amount\", col(\" total_amount\").cast(DoubleType()))\\\n",
    "    .drop(\" total_amount\")\\\n",
    "    .drop(\" vendor_id\")\\\n",
    "    .drop(\" payment_type\")\\\n",
    "    .drop(\" surcharge\")\\\n",
    "    .drop(\" mta_tax\")\\\n",
    "    .drop(\" tolls_amount\")\\\n",
    "    .drop(\" fare_amount\")\n",
    "\n",
    "def combine_raw_data():\n",
    "    info_title = lambda _y, _m : print(\"____________________________FOIL_COMBINE_{}_{}____________________________\".format(_y, _m))\n",
    "    info_start = lambda _y, _m : print(\"[SYSTEM]: Start  {}-{}\".format(_y, _m))\n",
    "    info_end   = lambda _y, _m : print(\"[SYSTEM]: Finish {}-{}\".format(_y, _m))\n",
    "    for _year in range(year_start, year_end + 1):\n",
    "        for _month in range(moth_start, moth_end + 1):\n",
    "            info_title(_year, _month)\n",
    "            tar_file = os.path.join(raws_dir , \"{}/{}.gz.parquet\".format(_year, _month))\n",
    "            tar_dbfs = os.path.join(raws_dbfs, \"{}/{}.gz.parquet\".format(_year, _month))\n",
    "            if check_file_exist(tar_file):\n",
    "                continue\n",
    "            fare_dbfs = os.path.join(csvs_dbfs, \"{}/trip_fare_{}.csv\".format(_year, _month))\n",
    "            trip_dbfs = os.path.join(csvs_dbfs, \"{}/trip_data_{}.csv\".format(_year, _month))\n",
    "\n",
    "            info_start(_year, _month)\n",
    "\n",
    "            _fare_df = spark.read.option(\"header\", True).csv(fare_dbfs)\n",
    "            _trip_df = spark.read.option(\"header\", True).csv(trip_dbfs)\n",
    "\n",
    "            _fare_df = process_fare(_fare_df)\n",
    "            _trip_df = process_trip(_trip_df)\n",
    "\n",
    "            rs_df = _trip_df.join(_fare_df, [\"medallion\", \"hack_license\", \"pickup_datetime\"])\n",
    "            rs_df.repartition(200)\\\n",
    "                .write.mode(\"overwrite\")\\\n",
    "                .option(\"compression\", \"gzip\")\\\n",
    "                .parquet(tar_dbfs)\n",
    "            info_end(_year, _month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________FOIL_COMBINE_2010_1____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/1.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_2____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/2.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_3____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/3.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_4____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/4.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_5____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/5.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_6____________________________\n",
      "[SYSTEM]: File exists: ../data/foil/raw/2010/6.gz.parquet\n",
      "____________________________FOIL_COMBINE_2010_7____________________________\n",
      "[SYSTEM]: Start  2010-7\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o337.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 14.0 failed 1 times, most recent failure: Lost task 8.0 in stage 14.0 (TID 293, 192.168.1.15, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:427)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.parquet.bytes.BytesInput$ByteArrayBytesInput.writeAllTo(BytesInput.java:449)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:346)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:198)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:75)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:275)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n\t... 9 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:273)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:427)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.parquet.bytes.BytesInput$ByteArrayBytesInput.writeAllTo(BytesInput.java:449)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:346)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:198)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:75)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:275)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n\t... 9 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:273)\n\t... 33 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-010b40717960>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcombine_raw_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-25-dc6b791bbab1>\u001B[0m in \u001B[0;36mcombine_raw_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m     68\u001B[0m                 \u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_fare_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"medallion\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"hack_license\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"pickup_datetime\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m                 \u001B[0;34m.\u001B[0m\u001B[0mcoalesce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0mrs_df\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m                 \u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"overwrite\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m                 \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"compression\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"gzip\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m    934\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_opts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 936\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    937\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    938\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o337.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 14.0 failed 1 times, most recent failure: Lost task 8.0 in stage 14.0 (TID 293, 192.168.1.15, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:427)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.parquet.bytes.BytesInput$ByteArrayBytesInput.writeAllTo(BytesInput.java:449)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:346)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:198)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:75)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:275)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n\t... 9 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:273)\n\t... 33 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:275)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:427)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:50)\n\tat org.apache.parquet.bytes.BytesInput$ByteArrayBytesInput.writeAllTo(BytesInput.java:449)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDictionaryPage(ParquetFileWriter.java:346)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:198)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:58)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:75)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:275)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\n\t... 9 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:273)\n\t... 33 more\n"
     ]
    }
   ],
   "source": [
    "combine_raw_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}