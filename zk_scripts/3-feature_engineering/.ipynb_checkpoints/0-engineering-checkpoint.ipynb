{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up modes and dirs\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "overwrite = True\n",
    "databricks = False\n",
    "is_yellow = False\n",
    "\n",
    "yellow = \"yellow\" if is_yellow else \"foil\"\n",
    "pick_up = \"pickup\"\n",
    "drop_off = \"dropoff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not databricks:\n",
    "    data_dir = \"/Users/kzmain/LSDE/data\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "else:\n",
    "    data_dir = \"/dbfs/mnt/group01\"\n",
    "clean_file  = clean_dbfs  = (data_dir + \"/{}\".format(yellow) + \"/cln/{}/{}.gz.parquet\")\n",
    "result_file = result_dbfs = (data_dir + \"/{}\".format(yellow) + \"/feature/{}/{}.gz.parquet\")\n",
    "\n",
    "if databricks:\n",
    "    clean_dbfs = clean_dbfs.replace(\"/dbfs\", \"\")\n",
    "    result_dbfs = result_dbfs.replace(\"/dbfs\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fr_year = 2009\n",
    "fr_month = 1\n",
    "\n",
    "to_year = 2017\n",
    "to_month = 12\n",
    "\n",
    "cluster_radian = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_file_exist(_path):\n",
    "    if os.path.exists(_path) and not overwrite:\n",
    "            print(\"[SYSTEM]: File exists: {}\".format(_path))\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cln(_year, _month):\n",
    "    return spark.read.parquet(clean_dbfs.format(_year, _month)).repartition(200, \"pick_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_duration(_in_df):\n",
    "    dur_col = \"duration_second\"\n",
    "    _in_df = _in_df.withColumn(dur_col, f.when(col(dur_col) > 4 * 60 * 60, False).otherwise(col(dur_col)))\n",
    "    return _in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_is_weekend(_in_df):\n",
    "    _in_df = _in_df.withColumn(\"is_weekend\", f.when(col(\"week_day\") > 5, f.lit(True)).otherwise(f.lit(False)))\n",
    "    return _in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_weather():\n",
    "    # |         time_stamp|hour|day|high|low|baro|wind| wd|hum|weather|year|month|\n",
    "    weather_dbfs = (data_dir + \"/nyc/weather/parquet\").replace(\"/dbfs\", \"\")\n",
    "    return spark.read.parquet(weather_dbfs).drop(\"time_stamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_weather(_in, _year):\n",
    "    _wh = get_weather().filter(col(\"year\") == _year)\n",
    "    _pm = \"pick_month\"\n",
    "    _pd = \"pick_day\"\n",
    "    _ph = \"pick_hour\"\n",
    "    _m = \"month\"\n",
    "    _d = \"day\"\n",
    "    _h = \"hour\"\n",
    "    _l = \"left_outer\"\n",
    "\n",
    "    return _in.join(_wh, (_in[_pm] == _wh[_m]) & (_in[_pd] == _wh[_d]) & (_in[_ph] == _wh[_h]), how=_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if is foil\n",
    "def feature_trip_num_in_day(_in_df):\n",
    "    _w1 = Window.partitionBy(\"medallion\", \"hack_license\", \"pick_month\", \"pick_day\").orderBy(\"pickup_datetime\")\n",
    "    _w2 = Window.partitionBy(\"medallion\", \"hack_license\", \"pick_month\", \"pick_day\")\n",
    "    _w_month = Window.partitionBy(\"medallion\", \"hack_license\", \"pick_month\")\n",
    "    _in_df = _in_df.withColumn(\"trip_prev_drop\", f.lag(_in_df[\"dropoff_datetime\"]).over(_w1)) \\\n",
    "        .withColumn(\"trip_cruise_second\", f.unix_timestamp(col(\"pickup_datetime\")) - f.unix_timestamp(col(\"trip_prev_drop\")))\n",
    "    _in_df = _in_df.withColumn(\"day_is_sleep_in_day\", f.when(col(\"trip_cruise_second\") > 4 * 60 * 60, True).otherwise(False)) \\\n",
    "        .withColumn(\"day_is_sleep_in_day\", f.max(col(\"day_is_sleep_in_day\")).over(_w2)) \\\n",
    "        .withColumn(\"day_sleep_second\", f.when(col(\"trip_cruise_second\") > 4 * 60 * 60, col(\"trip_cruise_second\")).otherwise(f.lit(0)))\n",
    "    _in_df = _in_df.withColumn(\"trip_cruise_second\", f.when(col(\"trip_cruise_second\") > 4 * 60 * 60, f.lit(None)).otherwise(col(\"trip_cruise_second\")))\n",
    "    _in_df = _in_df.withColumn(\"day_trip_count\",  f.count(col(\"pickup_datetime\")).over(_w2))\n",
    "    _in_df = _in_df.withColumn(\"day_start_stamp\", f.min(f.unix_timestamp(col(\"pickup_datetime\"))).over(_w2))\n",
    "    _in_df = _in_df.withColumn(\"day_end_stamp\"  , f.max(f.unix_timestamp(col(\"dropoff_datetime\"))).over(_w2))\n",
    "    _in_df = _in_df.withColumn(\"day_work_second\", col(\"day_end_stamp\") - col(\"day_start_stamp\") - f.sum(col(\"day_sleep_second\")).over(_w2))\n",
    "    _in_df = _in_df.withColumn(\"day_mean_tip_per_trip\", f.mean(\"tip_amount\").over(_w2)) \\\n",
    "        .withColumn(\"day_mean_travel_second_per_trip\", f.mean(\"duration_second\").over(_w2)) \\\n",
    "        .withColumn(\"day_mean_cruise_second_per_trip\", f.mean(\"trip_cruise_second\").over(_w2)) \\\n",
    "        .withColumn(\"month_income_total\", f.sum(\"total_amount\").over(_w_month)) \\\n",
    "        .withColumn(\"month_work_seconds\"  , f.sum(col(\"day_end_stamp\") - col(\"day_start_stamp\")).over(_w_month) - f.sum(col(\"day_sleep_second\")).over(_w_month)   ) \\\n",
    "        .withColumn(\"month_income_per_second\", f.col(\"month_income_total\") /  f.col(\"month_work_seconds\"))\\\n",
    "        .withColumn(\"month_work_on_weekend\", f.max(\"is_weekend\").over(_w_month))\n",
    "    return _in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "map_dir = os.path.join(data_dir, \"nyc/map\")\n",
    "nyc_map = ox.load_graphml(os.path.join(map_dir, \"NYC.mph\"))\n",
    "\n",
    "def get_node(lat, lon):\n",
    "    osmnx_id = ox.get_nearest_node(nyc_map, (lat,lon))\n",
    "    return int(osmnx_id)\n",
    "\n",
    "get_node_udf = f.udf(get_node, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_osmid(_in_df, _mode):\n",
    "    _c1_lat = \"{}_latitude\".format(_mode)\n",
    "    _c1_lon = \"{}_longitude\".format(_mode)\n",
    "    _c1_oid = '{}_osmid'.format(_mode)\n",
    "    _cor_df = _in_df.select([_c1_lat, _c1_lon]).dropDuplicates()\n",
    "    _cor_df = _cor_df.withColumn(_c1_oid, get_node_udf(_c1_lat, _c1_lon))\n",
    "    _in_df  = _in_df.join(_cor_df, [_c1_lat, _c1_lon])\n",
    "    return _in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for year in range(2009, 2020):\n",
    "    for month in range(1, 13):\n",
    "        if not os.path.exists(clean_file.format(year, month)):\n",
    "            continue\n",
    "        if check_file_exist(result_file.format(year, month)):\n",
    "            print(\"HAVE : {}-{}\".format(year, month))\n",
    "            continue\n",
    "        print(\"Start: {}-{}\".format(year, month))\n",
    "        print(\"clean\")\n",
    "        cln = get_cln(year, month)\n",
    "        print(\"[System]: Feature duration\")\n",
    "        cln = filter_duration(cln)\n",
    "        print(\"[System]: Feature is weekend\")\n",
    "        cln = feature_is_weekend(cln)\n",
    "        print(\"[System]: Feature weather\")\n",
    "        cln = feature_weather(cln, year)\n",
    "        print(\"feature_weather\")\n",
    "        cln = get_osmid(cln, pick_up)\n",
    "\n",
    "        if not is_yellow:\n",
    "            cln = feature_trip_num_in_day(cln)\n",
    "\n",
    "        cln.write.mode(\"overwrite\").option(\"compression\", \"gzip\")\\\n",
    "            .partitionBy(\"pick_day\").parquet(result_dbfs.format(year, month))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
